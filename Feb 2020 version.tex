\documentclass[14pt, conference]{IEEEtran}
\ifCLASSINFOpdf
\else
\fi

\IEEEoverridecommandlockouts

\usepackage{listings}
\usepackage[table, svgnames]{xcolor}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.5,1,0.5}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=none,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}


\usepackage{float}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage{multirow}
%\usepackage{subcaption}
\usepackage{flushend}
\usepackage{hyperref}
\usepackage{tabularx} 
\usepackage{booktabs} % For formal tables
\usepackage{hhline}
\usepackage{array}

\colorlet{headercolour}{DarkSeaGreen}
\AtBeginEnvironment{tabular}{\rowcolors{1}{\ifnumequal{\rownum}{1}{headercolour}{white}}{}}%

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\bibliographystyle{IEEEtran}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\hypersetup{bookmarks=false}

\begin{document}

\title{Network Anomaly Detection Using LightGBM: A Gradient Boosting Classifier}

\author{
\IEEEauthorblockN{Md. Khairul Islam\textsuperscript{1},
Prithula Hridi\textsuperscript{1}, Md. Shohrab Hossain\textsuperscript{1}, Husnu S. Narman\textsuperscript{2}}

\IEEEauthorblockA{\textsuperscript{1}Department of Computer Science and Engineering, Bangladesh University of Engineering and Technology, Bangladesh\\
    \textsuperscript{2}Weisberg Division of Computer Science, Marshall University, Huntington, WV, USA\\}
\IEEEauthorblockA{Email:  khairulislamtanim@gmail.com, prithula5117@gmail.com, mshohrabhossain@cse.buet.ac.bd,  narman@marshall.edu}
}

\maketitle

\begin{abstract}
Anomaly detection system plays a significant role in recognizing intruders or suspicious activities inside the system,
catching unseen and unknown attacks. In this paper, we have worked on a benchmark data set UNSW-NB15, that
reflects modern-day network traffics. We used a machine learning classifier LightGBM to perform binary classification
on the dataset. We used different experimentation setups following prior arts. Using ten-fold cross-validation on train,
test and combined(train+test) dataset our model achieved 97.20\%, 98.33\% and 96.21\% f1\_score respectively. Also, the
 model fitted only on train data, achieved 92.96\% f1\_score on the separate test set. Comparing with prior arts,
 we found our model outperforms them in most metrics.
\end{abstract}

\begin{IEEEkeywords}
anomaly detection, machine learning,  network security.
\end{IEEEkeywords}
%------------------------ Into \input{introduction.tex}

\section{Introduction}
As web applications are being increasingly popular, the Internet has become a necessity in our day-to-day life. As a
consequence, network systems are being targeted more by attackers with malicious intent. To detect intruders in a
network system, there are generally two approaches: signature-based and anomaly-based approaches. Signature-based
systems maintain a database of previously known attacks and raise alarms if when any match is found with the analyzed
 data. However, they are vulnerable to zero-day attacks.

An anomaly in a network means a deviation of traffic data from its normal pattern. Thus, anomaly detection techniques
have the advantage of detecting zero-day attacks. However, in a complex and large network system,  it is not easy to
define a set of valid requests or normal behavior of the endpoints. Hence, anomaly detection faces the disadvantage of
having a high false-positive error rate (events erroneously classified as attacks). There are different types of
anomalies that can be mapped with different types of attacks. According to Ahmed et al. \cite{ahmed2016survey},
the main attack types are DoS, Probe, U2R (User to Root) and R2U (Remote to User) attack. They mapped the point
anomaly with the U2R \& the R2U attacks, DoS attack to the collective anomaly and Probe attack to the contextual anomaly.

In recent years, machine learning and deep learning have become increasingly popular. They have been applied to
different anomaly and intrusion detection systems \cite{kwon2017survey}, \cite{naseer2018enhanced},
\cite{fernandes2019comprehensive}, \cite{chalapathy2019deep}. In many cases, they have outperformed the previous
state-of-the-art models.

As UNSW-NB15\cite{moustafa2015unsw} is a benchmark anomaly detection dataset, numerous studies have been performed on it.
However, as shown in Section \ref{relatedWorks}, to evaluate the same dataset different setups were adopted.
Also, works that followed the same experimentation setup did not compare their performance with prior arts in many cases.
Which makes it hard to validate their improvements. Some works mentioned near-perfect scores which makes us wonder if
they have any limitations. Some only mentioned accuracy or few metrics which made it hard for future researchers to
compare their performance with them or judge the overall improvement. So in this work we

\begin{itemize}
    \item provided a thorough study of the UNSW-NB15 dataset with feature preprocessing, selection, parameter hyper tuning.
    \item explored the performance of a boosting algorithm in binary classification on the dataset following
    experimentation setups done in previous studies. The performance of boosting algorithms is yet to be explored
    thoroughly on this dataset.
    \item mentioned all related performance metrics and compared our results with prior arts. We found our model
    outperforms previous results in most metrics.
\end{itemize}

The rest of the paper is organized as follows. In Section \ref{relatedWorks} we mention the recent works in the field
related to NIDS. Details about the dataset preprocessing, feature engineering and performance metrics are given in
Section \ref{methodology}. In Section \ref{results} we describe the evaluation process and results. We also showed
some comparisons with prior arts with the results in this section. The rest of the comparisons are shown in Section
\ref{comparison} . Finally, Section \ref{conclusion} has the concluding remarks.


\section{Related works \label{relatedWorks}}
For network intrusion detection KDDCUP99, NSL-KDD, DARPA, UNSW-NB15 are among the benchmark dataset. So numerous
works can be found on the binary classification on it. Moustafa et al. \cite{moustafa2015significant} studied the
significant features of the UNSW-NB15 and KDD99 dataset. The results show the original KDD99 attributes are less
efficient than the replicated UNSW-NB15 attributes of the KDD99 data set.  Moustafa et al. \cite{moustafa2016evaluation}
also analyzed the statistical properties of the UNSW-NB15 dataset. The complexity of the dataset was  evaluated using
five techniques (DT, LR, NB, ANN and EM clustering). Here, based on the performance results UNSW-NB15 was found to be
more complex compared to the KDD99 dataset.

In \cite{moustafa2017hybrid} authors used central points of attribute values and Association Rule Mining for feature
selection on a high level of abstraction from datasets UNSW-NB15 and NSL-KDD. They ranked highest generated attributes
from the association rule and choose top 11 of them. For any less number of features than this their evaluation
results were extremely unsatisfactory. They evaluated performance using Expectation-Maximisation clustering (EM),
the Logistic Regression (LR) and the Naïve Bayes (NB), in terms of accuracy and False Alarm Rate (FAR). LR produced
the best results on the two datasets with accuracy and FAR, 83\% and 14.2\% on UNSW-NB15 dataset, 82.1\% and 17.5\% on
NSL-KDD dataset.


Mogal et al. \cite{mogal2017nids} used machine learning classifiers on both UNSW-NB15 and KDDCUP99 datasets.
They achieved nearly 100\% accuracy on both datasets using Naïve Bayes and Logistic Regression on train data.
Koroniotis et al.\cite{koroniotis2017towards} selected the top ten features of the UNSW-NB15 combined(train+test)
dataset using Information Gain Ranking Filter. Then ran ten-fold cross-validations using machine learning techniques.
Among the four techniques(ARM, DT, NB, ANN) applied, DT(Decision Tree C4.5 Classifier) performed the best at
 distinguishing between Botnet and normal network traffic. It achieved 93.23\% accuracy and 6.77\% false alarm rate.
 Meghdouri et al. \cite{meghdouri2018analysis} applied feature preprocessing and principal component analysis on the
 UNSW-NB15 dataset. Then performed five-fold cross-validation using a RandomForest classifier and achieved 84.9\%
 f-measure.

Viet et al. \cite{viet2018using} used the UNSW-NB15 and NSL-KDD datasets only to detect network scanning attacks.
Scanning attacks in NSL-KDD dataset are IPSweep, Nmap, PortSweep and Satan. In UNSW-NB15 as the scanning types are
labelled altogether they applied binary classification. Using Deep Belief Network, they achieved true positive rate
and false alarm rate, 99.86\% and 2.76\%  on UNSW-NB15 dataset, 99.458\% and 2.71\% on NSL-KDD dataset.

Suleiman et al. \cite{suleiman2018performance} explored the performance of machine learning classifiers on benchmark
and new dataset (UNSW-NB15, NSL-KDD, and Phishing) using ten-fold cross-validation. They found the RandomForest
classifier to perform best on the NSL-KDD dataset with accuracy 99.76\%, on UNSW-NB15 with accuracy 90.14\%. For
the Phishing dataset J48 classifier performed best with accuracy 90.76\%. All the experiments were done using the
WEKA tool. Belouch et al. \cite{belouch2018performance} used machine learning classifiers to detect network intrusions
on the UNSW-NB15 dataset on Apache Spark. Moustafa et al. \cite{moustafa2018anomaly} proposed a beta mixture
model-based anomaly detection system on the UNSW-NB15 dataset. They first selected eight features from the dataset,
then randomly selected samples from the dataset. The best result had accuracy 93.4\%, detection rate 92.7\% and
false positive rate 5.9\%. In other work \cite{moustafa2019holistic} the authors selected random samples from
the UNSW-NB15 dataset and ran ten-fold cross-validation on it. They found the LogisticRegression classifier to
achieve the best result 95.6\% accuracy and 5.6\% false alarm rate.

Nawir et al. \cite{nawir2019effective} applied ten-fold cross-validation on the binary classification of the combined
(train+test) dataset by using the WEKA tool. They also compared centralized and distributed AODE algorithms based on
accuracy against the number of nodes. Kanimozi et al. \cite{Kanimozhi2019UNSW-NB15} chose the best four features of
the UNSW-NB15 dataset using the RandomForest classifier. They also used a MultiLayerPerceptron to show how neural
networks would perform on this dataset.  Meftah et al. \cite{meftah2019network} applied both binary and multiclass
classification on the UNSW-NB15 dataset. They found for binary classification SVM performs the best in ten-fold
cross-validation with 82.11\% accuracy and decision tree (C5.0) perform the best for multiclass classification with
86\% f-measure and 85.41\% accuracy on train data. Hanif et al. \cite{hanif2019intrusion} used ANN(Artificial Neural Network)
on the same dataset. The neural network had one hidden layer and it achieved an average 84\% accuracy and less
than 8\% false-positive rate in repeated cross-validation. They compared their performance with prior works on the
NSL-KDD dataset, instead of works on the same dataset.


%-------------------------------------------------------------

\section{Proposed Methodology \label{methodology}}
We target only to perform binary classification on the dataset. We used Kaggle kernels for running our models.
It provides with 4 CPU cores, 16 Gigabytes of RAM. In following subsections we describe how we prepared the dataset
for experimentation and performance metrics used for model evaluation.


\subsection{Dataset Description}
We have used the UNSW-NB15 dataset\cite{moustafa2015unsw} which is a recent benchmark for NIDS.
It was designed at the Cyber Range Lab of the Australian Center of Cyber Security. Compared to other existing datasets
like KDDCup99, NSL-KDD, DARPA this dataset is more recent and better reflects modern network traffic.
UNSW-NB15 represents nine major families of attacks by utilizing the IXIA PerfectStorm tool. The attack types are
fuzzers, analysis, backdoor, dos, exploit, generic, reconnaissance, shellcode and worm. The dataset has 45 features
including the target label (0 for normal, 1 for attack), attack\_cat and id (unique number for each record).
So only 42 columns can be considered as features. We considered binary classification for this study.
So we only predict whether the record is attack type or normal. The distribution of target labels in both train
and test data is given in table \ref{datasetDescription}.

\begin{table}
\normalsize
\centering
\caption{Dataset Description}
\label{datasetDescription}
\renewcommand{\arraystretch}{1.2}

\begin{tabular}{|C{2cm}|C{1.8cm}|C{1.8cm}|}
\hline
 \textbf{Type} & \textbf{Train} & \textbf{Test} \\ \hline
Normal & 56,000 & 37,000 \\ \hline
Anomaly & 119,341 & 45,332 \\ \hline
\textbf{Total}  & \textbf{175,341} & \textbf{82,332} \\ \hline
\end{tabular}
\end{table}


\subsection{Preprocessing \label{preprocessing}}
We have performed the preprocessing on the data set using the following steps :

\subsubsection{Dropping unnecessary columns}
We have dropped ID column data as it can not be learned as a feature. Also, attack\_cat as we are not doing multi-class
classification. And label column is our target. So among 45 features, 42 were left after dropping these three.

\subsubsection{Feature engineer categorical columns}
We found many categorical labels to have very low frequency. To make it easier for the model to learn from these
categorical features we converted features with low frequency values into a single label. For state feature, except
the top five labels by frequency ('FIN', 'INT', 'CON', 'REQ', 'RST') other labels were converted into label 'others'.
Similarly, for service column labels except '-', 'dns', 'http', 'smtp', 'ftp-data', 'ftp', 'ssh', 'pop3' were converted
into 'others' label. For proto column 'igmp', 'icmp', 'rtp' labels were combined into label 'igmp\_icmp\_rtp'.
Then labels except 'tcp', 'udp', 'arp', 'ospf', 'igmp\_icmp\_rtp' were converted into label 'others'.


\subsubsection{Scaling}
We have applied StandardScaler from sklearn's preprocessing library on all non-categorical features. It was
fitted on the train data, then the fitted scaler was used to covert both train and test data. It converts values
using the following equation:
\begin{equation}
    x = \frac{x-\mu}{\sigma}
\end{equation}

where $\mu$ is the mean value and $\sigma$ is the standard deviation.

\subsubsection{Feature Selection}
We used the RandomForest classifier of sklearn with default parameters to calculate feature importance on the
train dataset. We first preprocessed dataset using previous steps. Then averaged feature importance over ten-fold
cross-validation. We convert the values into percentages, for easier understanding. Then sorted them in descending order.
From there we chose to drop features with less than 0.5\% importance value. The dropped 6 features are response\_body\_len,
is\_sm\_ips\_ports, ct\_flw\_http\_mthd, trans\_depth, dwin, ct\_ftp\_cmd, is\_ftp\_login. In table \ref{featureImportance}
we have shown the chosen features with corresponding importance.

\begin{table}
\normalsize
\centering
\caption{Feature Importance}
\label{featureImportance}
% increases cell padding
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|C{2.2cm}|C{1.2cm}|C{2.5cm}|C{1.2cm}|}
\hline
\textbf{Feature} & \textbf{Importance} & \textbf{Feature} & \textbf{Importance}\\ \hline

sttl & 15.23& dur & 1.87  \\ \hline
ct\_state\_ttl & 13.81& dpkts & 1.65  \\ \hline
dload & 6.28& sjit & 1.54  \\ \hline
dttl & 5.85& djit & 1.32  \\ \hline
rate & 4.61& state & 1.3  \\ \hline
ackdat & 4.52& ct\_dst\_ltm & 1.09  \\ \hline
sload & 3.82& proto & 0.93  \\ \hline
sbytes & 3.75& dtcpb & 0.88  \\ \hline
dmean & 3.18& dloss & 0.86  \\ \hline
ct\_srv\_dst & 2.69& spkts & 0.77  \\ \hline
dbytes & 2.65& stcpb & 0.75  \\ \hline
dinpkt & 2.64& ct\_src\_ltm & 0.73  \\ \hline
smean & 2.51& sloss & 0.72  \\ \hline
synack & 2.44& is\_sm\_ips\_ports & 0.61  \\ \hline
ct\_dst\_src\_ltm & 2.17& ct\_src\_dport\_ltm & 0.6  \\ \hline
ct\_srv\_src & 2.11& swin & 0.56  \\ \hline
tcprtt & 2.06& service & 0.43  \\ \hline
sinpkt & 1.93& ct\_dst\_sport\_ltm & 0.41  \\ \hline

\end{tabular}
\end{table}


\subsubsection{OneHotEncoding}
We used pandas library to OneHotEncode all the categorical features. So the final number of features is 53.


\subsection{Evaluation metrics}
Here, we discuss all the performance metrics used to compare the performance of our approach with previous works.
All the metrics were calculated using the sklearn.metrics module.
\begin{itemize}
    \item \textbf{True Positives (TP)}: The cases in which we predicted YES and the actual output was also YES.
    \item \textbf{True Negatives (TN}: The cases in which we predicted NO and the actual output was NO.
    \item \textbf{False Positives (FP)}: The cases in which we predicted YES and the actual output was NO.
    \item \textbf{False Negatives (FN)}: The cases in which we predicted NO and the actual output was YES.
\end{itemize}

\subsubsection{Accuracy}
The ratio of the number of correct predictions to the total number of input samples
\begin{equation}
    accuracy = \frac{TP+TN}{TP+TN+FP+FN}
\end{equation}

\subsubsection{Precision}
The number of correct positive results divided by the number of positive results predicted by the classifier.
\begin{equation}
    precision = \frac{TP}{TP+FP}
\end{equation}

\subsubsection{Recall or Sensitivity or DR(Detection Rate) or TPR (True Positive Rate)}
The number of correct positive results divided by the number of all relevant samples (all samples that should
have been identified as positive)
\begin{equation}
    recall = \frac{TP}{TP+FN}
\end{equation}

\subsubsection{F1\_score}
The harmonic Mean between precision and recall.
\begin{equation}
    f1\_score = 2 * \frac{1}{\frac{1}{precision}+ \frac{1}{recall}}
\end{equation}

\subsubsection{FPR (False Positive Rate)}
The proportion of incorrectly identified observations.
\begin{equation}
    FPR = \frac{FP}{FP+TN}
\end{equation}

\subsubsection{FAR (False Alarm Rate)}
Represents the probability that a record gets incorrectly classified.
\begin{equation}
    FAR = \frac{FP+FN}{FP+FN+TP+TN}
\end{equation}

\subsubsection{ROC AUC}
It computes Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.

\subsubsection{Time (sec)}
As run time is an important factor in detecting anomalies, we have also mentioned them for our models. We used the time
library of python for this purpose.


\section{Experiment and Results \label{results}}
For evaluating the UNSW-NB15 dataset, we did ten-fold cross validation on the train dataset using Stratified KFold of
the sklearn library with random shuffle true. We used several popular machine learning classifiers to measure the
detection performance. The models were run mostly with default parameters. We set random state to 1 for all of them,
so that the results are reproducible. All models except LightGBM \cite{ke2017lightgbm}, were from sklearn library
version 0.23.0. During prediction, for LightGBM used the best iteration. The used models are listed below with
important parameters.
\begin{enumerate}
    \item LogisticRegression : penalty = l2, max\_iter = 100, solver = lbfgs, C = 1.0
    \item GradientBoosting: learning\_rate = 0.1, n\_estimators = 100,max\_depth = 3
    \item DecisionTree: criterion = 'gini', max\_depth = None, max\_features = None,
    \item RandomForest: n\_estimators = 100, criterion = 'gini', max\_depth = None, max\_features = None
    \item LightGBM: learning rate = 0.1, objective = binary, metric = binary\_logloss,boost\_from\_average = True,
    num\_round = 2000, early\_stopping\_rounds = 50.
\end{enumerate}

Then we chose the best model based on accuracy and f1-score. From table \ref{crossvalidationWithDifferentModels}
LightGBM showed the best performance in both accuracy (96.18\%) and f1-score (91.21\%).

\begin{table}
\normalsize
\centering
\caption{Ten-fold cross validation with different models}
\label{crossvalidationWithDifferentModels}
\renewcommand{\arraystretch}{1.2}

\begin{tabular}{|C{2.8cm}|C{2.1cm}|C{2.1cm}|}
\hline
\textbf{Metrics(\%)} & \textbf{Accuracy(\%)} & \textbf{F1\_score(\%)} \\ \hline
LogisticRegression & 93.54 & 95.42 \\ \hline
GradientBoosting & 94.58 & 96.11\\ \hline
DecisionTree  & 94.99 & 96.32\\ \hline
RandomForest  & 96.08 & 97.14 \\ \hline
LighGBM & 96.18 & 97.21 \\ \hline
\end{tabular}
\end{table}


\subsection{Evaluation on train data}
Mogal et al. \cite{mogal2017nids}, Kanimozhi et al. \cite{Kanimozhi2019UNSW-NB15} evaluated model performance on the
UNBSW-NB15 dataset without using any cross-validation approach. The same data used for training the model was used for
validation too. To compare our model's performance with them, we also followed a similar setup. As evident from the
results shown in table \ref{evaluationOnTrainData}, this type of experimentation setup does not truly reflect model
performance. As the model overfits on train data, its performance will become very poor on a separate test set. For
example, we found our model when overfitted on train data, only achieved 86.88\% accuracy and 89.14\% f-measure on
test data.

\begin{table}
\normalsize
\centering
\caption{Evaluating on train data}
\label{evaluationOnTrainData}
\renewcommand{\arraystretch}{1.2}

\begin{tabular}{|C{3.5cm}|C{1.8cm}|C{1.8cm}|}
\hline
\textbf{Metrics(\%)} & \textbf{Train data} & \textbf{Test data} \\ \hline
Accuracy & 99.60 & 99.98 \\ \hline
Precision & 99.52 & 99.97\\ \hline
Recall  & 99.89 & 99.98\\ \hline
F1\_score  & 99.71 & 99.98 \\ \hline
FPR & 0.01 & 0.0004\\ \hline
AUC & 99.99 & 99.99\\ \hline
Time(s) & 243 & 237\\ \hline
\end{tabular}
\end{table}


\subsection{Ten-fold cross validation}
Ten-fold cross-validation on train, test or combined(train+test) dataset was performed in \cite{meftah2019network},
\cite{suleiman2018performance}, \cite{nawir2019effective}, \cite{hanif2019intrusion} . We used the StratifiedKFold
method of sklearn.model\_selection module with shuffle = True to perform the ten-fold cross validation. Average
scores achieved in that process is shown in table \ref{tenFoldCrossValidation}.

\begin{table}
% this increases the fontsize used in table
\normalsize
\centering
\caption{Ten-fold cross validation}
\label{tenFoldCrossValidation}
% increases cell padding
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|C{2.3cm}|C{1.3cm}|C{1.3cm}|C{1.8cm}|}
\hline
\textbf{Metrics(\%)} & \textbf{Train} & \textbf{Test} & \textbf{Combined}\\ \hline
Accuracy & 96.18 & 98.18 & 95.19 \\ \hline
Precision & 96.54 & 98.87& 96.84\\ \hline
Recall  & 97.89 & 97.80 & 95.58\\ \hline
F1\_score  & 97.21 & 98.33 & 96.21\\ \hline
FPR & 7.47 & 1.37 & 5.51\\ \hline
FAR & 3.82 & 1.83 & 4.81 \\ \hline
AUC & 99.45 & 99.82 & 99.26\\ \hline
Time(s) & 628.1 & 281.1 & 838.8\\ \hline
\end{tabular}
\end{table}


\subsection{Validation on test data}
In this phase we validated the model trained on train data using the separate test dataset of UNSW-NB15 following
\cite{bhamare2016feasibility} \cite{vinayakumar2019deep} \cite{dahiya2018network}. As  Meftah et al. \cite{meftah2019network}
mentioned, some columns have new labels in test data. However, after our feature engineering process in section
\ref{preprocessing}, we were able to overcome it. For this evaluation specifically, we found setting parameter
is\_unbalance: True and learning rate to 0.05 in LightGBM improved prediction performance. The performance metrics
are shown in table \ref{validationResult} along with comparisons with prior arts. In table \ref{confusionMatrix}
we have showed the normalized confusion matrix.

\begin{table}
\normalsize
\centering
\caption{Validation on test data}
\label{validationResult}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|C{2cm}|C{1.2cm}|C{1.3cm}|C{1.7cm}|}
\hline
\textbf{Metrics(\%)} & \textbf{Ours} & \textbf{RF\cite{vinayakumar2019deep}} & \textbf{REP Tree\cite{dahiya2018network} }\\ \hline
Accuracy & 91.95 & 90.3 & 93.56 \\ \hline
Precision & 89.59 & 98.8 & 83.3\\ \hline
Recall  & 96.60 & 86.7 & 83.2 \\ \hline
F1\_score  & 92.96 & 92.4 & 83.25 \\ \hline
FPR & 13.75 & - & 2.3 \\ \hline
FAR & 8.05 & - &  - \\ \hline
AUC & 98.64 & - & - \\ \hline
Time(s) & 31.44 & - & - \\ \hline

\end{tabular}
\end{table}

Our model outperforms the work of Vinayakumar et al \cite{vinayakumar2019deep} by both accuracy and f1\_score.
Though Dahiya et al \cite{dahiya2018network} achieved better accuracy than ours, they had near 10\% drop in
1\_score than our model. In intrusion detection dataset where class distribution is imbalanced, f1\_score is more important.

\begin{table}
\normalsize
\centering
\caption{Confusion matrix}
\label{confusionMatrix}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|C{2.2cm}|C{2.3cm}|C{2.3cm}|}
\hline
 & Predicted Normal & Predicted Anomaly \\ \hline
Actual Normal & 0.86 & 0.14 \\ \hline
Actual Anomaly & 0.03 & 0.97\\ \hline
\end{tabular}
\end{table}


\section{Comparison \label{comparison}}
\subsection{Evaluation on train data}
Mogal et al. \cite{mogal2017nids} achieved 99.96\% accuracy on this dataset using Naive Bayes and LogisticRegression,
which did not follow any cross-validation approach. Belouch et al. \cite{belouch2018performance} found RandomForest
classifier to achieve 97.49\% accuracy on this dataset. A similar approach was taken by Kanimozhi et al.
\cite{Kanimozhi2019UNSW-NB15} with the best four features chosen using the RandomForest classifier.
The model achieved 98.3\% accuracy. It is shown in table \ref{evaluationOnTrainData} that in the same validation
process our model achieves near-perfect scores on the train and test data. We also did not find any comparison to
prior art with similar validation process \cite{mogal2017nids}, in \cite{belouch2018performance} and
\cite{Kanimozhi2019UNSW-NB15}.

\subsection{Ten-fold cross validation}
Suleiman et al. \cite{suleiman2018performance} evaluated performance using ten-fold cross validation on train data.
They found a RandomForest (RF) classifier to have the best accuracy and f-measure. We mention our model performance
using same validation process in column train of table \ref{tenFoldCrossValidation}. TPR and recall are same,
so we mention only recall.
\begin{table}
\normalsize
\centering
\caption{Performance comparison with \cite{suleiman2018performance}}
\label{performanceComparisonWithSuleiman}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|C{2.3cm}|C{2.2cm}|C{2.2cm}|}
\hline
\textbf{Metrics(\%)} & \textbf{RF \cite{suleiman2018performance}} & \textbf{LightGBM} \\ \hline
Accuracy & 90.14 & 96.17\\ \hline
Precision  & 99.8 & 96.54\\ \hline
Recall  & 97.8 & 97.89\\ \hline
F1\_score  & 98.7 & 97.20\\ \hline
FPR  & 0.1 & 7.48\\ \hline
\end{tabular}
\end{table}

Meftah et al. \cite{meftah2019network} applied ten-fold cross validation on train dataset and achieved the best accuracy
82.11\% using SVM classifier. In same validation process our accuracy is 96.17\%. Hanif et al. \cite{hanif2019intrusion}
applied ten-fold cross validation on train and test dataset repeatedly using Artificial Neural Network(ANN) and achieved
average 84\% accuracy, 8\% false positive rate. In similar case, as shown in table \ref{tenFoldCrossValidation},
our performance is better. Though \cite{meftah2019network}\cite{hanif2019intrusion} followed the same experimentation
setup as prior art \cite{suleiman2018performance}, they did not present any comparisons.

Koroniotis et al. \cite{koroniotis2017towards} performed ten-fold cross validation on the combined dataset.
The best result was achieved using the Decision Tree C4.5. We mentioned our model performance using similar
validation in column combined of table \ref{tenFoldCrossValidation}.

\begin{table}
\normalsize
\centering
\caption{Comparison with Koroniotis et al. \cite{koroniotis2017towards}}
\label{performanceComparisonWithKoroniotis}
\renewcommand{\arraystretch}{1.2}

\begin{tabular}{|C{3.3cm}|C{2.2cm}|C{1.5cm}|}
\hline
\textbf{Classifier} & \textbf{Accuracy (\%)} & \textbf{FAR(\%)} \\ \hline
Decision Tree \cite{koroniotis2017towards} & 93.23 & 6.77 \\ \hline
LightGBM & 95.19 & 4.81 \\ \hline
\end{tabular}
\end{table}

Nawir et al. \cite{nawir2019effective} applied a similar ten-fold cross-validation evaluation on the combined dataset
using the WEKA J48 classifier. They mentioned achieving high accuracy of 98.71\% using the default parameter.
However, using the same environment we found it achieves 94.61\% accuracy, which is lower than our model's
performance (95.19\%).

\subsection{Validation on test data}
Bhamare et al. \cite{bhamare2016feasibility} achieved acc 89.26\%, TP 93.7\% and TN 95.7\% at prediction threshold 0.5.
Increasing prediction threshold to 0.7-0.8 their TP improved to 97\%, but TN dropped to 80\%. Where our TP and
TN are 97\% and 86\% at threshold 0.5 as shown in table \ref{confusionMatrix}.

\subsection{Others}
We found only Meghdouri et al. \cite{meghdouri2018analysis} to validate using five-fold cross-validation.
So we did not add any separate section for it, however we present our model performance using same validation process
here in table \ref{performanceComparisonWithMeghdouriTrain} and \ref{performanceComparisonWithMeghdouriTest}.
Table \ref{performanceComparisonWithMeghdouriTrain} shows our model performance compared to their's on five-fold
cross validation of train dataset.

Their model achieved higher accuracy (99\%) compared to ours (96.18\%). However, for precision, recall and
f1\_score we see our model performance is much higher. For same validation process on test dataset from
table \ref{performanceComparisonWithMeghdouriTest} we showed that our test accuracy is very close to theirs.
However, as before our precision, recall and f1\_score achieved much better than their's. Our ROC AUC
scores are very close too. However, for intrusion detection techniques f-measure is very important,
in which our model outperforms by a large margin.

\begin{table}
\normalsize
\centering
\caption{Comparison with Meghdouri et al.\cite{meghdouri2018analysis} (Train data)}
\label{performanceComparisonWithMeghdouriTrain}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|C{3cm}|C{2cm}|C{2cm}|}
\hline
\textbf{Metrics(\%)} & \textbf{Train\cite{meghdouri2018analysis}} & \textbf{Train} \\ \hline
Accuracy & 99.0 & 96.18\\ \hline
Precision  & 85.9 & 96.56 \\ \hline
Recall  & 85.1 & 97.87 \\ \hline
F1\_score  & 84.9 & 97.21 \\ \hline
ROC AUC  & 99.8 & 99.44 \\ \hline
\end{tabular}
\end{table}

\begin{table}
\normalsize
\centering
\caption{Comparison with Meghdouri et al.\cite{meghdouri2018analysis} (Test data)}
\label{performanceComparisonWithMeghdouriTest}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|C{3cm}|C{2cm}|C{2cm}|}
\hline
\textbf{Metrics(\%)} & \textbf{Test\cite{meghdouri2018analysis}} & \textbf{Test} \\ \hline
Accuracy & 98.9 & 98.08\\ \hline
Precision  & 84.9 &  98.79\\ \hline
Recall & 85.1 & 97.7\\ \hline
F1\_score & 84.9 & 98.24 \\ \hline
ROC AUC   & 99.8& 99.81\\ \hline
\end{tabular}
\end{table}

We could not compare with some prior arts. Despite having a separate train and test dataset many of them chose to
randomly sample train and test data.  For example Moustafa et al. \cite{moustafa2018anomaly} \cite{moustafa2019holistic}
evaluated the model on randomly chosen data from UNSW-NB15 dataset. However, it is not possible to reproduce a random dataset.


\section{Conclusion \label{conclusion}}
In this paper, we presented a boosting algorithm-based model for performing binary classification of the UNSW-NB15 dataset.
We explained all the steps taken from feature preprocessing, selection and validation of the model. We also
followed different experimentation setups to compare our performance with prior arts. In Section \ref{comparison} we
showed our trained model outperforms prior arts in most metrics. We also showed why experimentation setups followed by
some prior arts are heavily overfitted and should be avoided. Our model also performed well on test data when
it is fitted on train data only. Which validated the generalization of our model.  We also provided detailed performance
 of our model to prove our claims. This study only performs a binary classification. In the future we want to improve
 the performance of multiclass-classification on this dataset similarly.


\bibliography{bibliography}
\end{document}


